{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d995ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils.path_converter import path_converter\n",
    "\n",
    "df = pd.read_csv(path_converter(\"/data/raw/dates_100k.csv\"))\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.to_csv(path_converter(\"/data/raw/train_dates.csv\"), index=False)\n",
    "val_df.to_csv(path_converter(\"/data/raw/val_dates.csv\"), index=False)\n",
    "test_df.to_csv(path_converter(\"/data/raw/test_dates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e54987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Generating train split: 81000 examples [00:00, 1161257.39 examples/s]\n",
      "Generating val split: 9000 examples [00:00, 1423138.02 examples/s]\n",
      "Generating test split: 10000 examples [00:00, 1344069.73 examples/s]\n",
      "Map: 100%|██████████| 81000/81000 [00:10<00:00, 7829.13 examples/s]\n",
      "Map: 100%|██████████| 9000/9000 [00:01<00:00, 8174.10 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7918.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], max_length=64, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], max_length=16, padding=\"max_length\", truncation=True\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": path_converter(\"/data/raw/train_dates.csv\"),\n",
    "        \"val\": path_converter(\"/data/raw/val_dates.csv\"),\n",
    "        \"test\": path_converter(\"/data/raw/test_dates.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223822aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"date-normalizer\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=200,\n",
    "    use_cpu=True,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"],\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=\"date-normalizer/checkpoint-3000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ccdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-12-01\n"
     ]
    }
   ],
   "source": [
    "def normalize_date(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=20, num_beams=5)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(normalize_date(\"2010-12-01 08:26:00\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8f0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    i = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        i += 1\n",
    "        inp = tokenizer(sample[\"input_text\"], return_tensors=\"pt\")\n",
    "        output = model.generate(**inp, max_length=20)\n",
    "        prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        if prediction == sample[\"target_text\"]:\n",
    "            correct += 1\n",
    "        print(correct / i)\n",
    "\n",
    "    print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd58c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9\n",
      "0.9090909090909091\n",
      "0.9166666666666666\n",
      "0.9230769230769231\n",
      "0.9285714285714286\n",
      "0.9333333333333333\n",
      "0.9375\n",
      "0.9411764705882353\n",
      "0.8888888888888888\n",
      "0.8421052631578947\n",
      "0.85\n",
      "0.8095238095238095\n",
      "0.7727272727272727\n",
      "0.782608695652174\n",
      "0.75\n",
      "0.76\n",
      "0.7692307692307693\n",
      "0.7777777777777778\n",
      "0.7857142857142857\n",
      "0.7931034482758621\n",
      "0.7666666666666667\n",
      "0.7741935483870968\n",
      "0.78125\n",
      "0.7575757575757576\n",
      "0.7647058823529411\n",
      "0.7714285714285715\n",
      "0.7777777777777778\n",
      "0.7837837837837838\n",
      "0.7894736842105263\n",
      "0.7948717948717948\n",
      "0.775\n",
      "0.7804878048780488\n",
      "0.7857142857142857\n",
      "0.7906976744186046\n",
      "0.7954545454545454\n",
      "0.8\n",
      "0.8043478260869565\n",
      "0.8085106382978723\n",
      "0.7916666666666666\n",
      "0.7959183673469388\n",
      "0.8\n",
      "0.803921568627451\n",
      "0.7884615384615384\n",
      "0.7924528301886793\n",
      "0.7962962962962963\n",
      "0.8\n",
      "0.8035714285714286\n",
      "0.8070175438596491\n",
      "0.8103448275862069\n",
      "0.8135593220338984\n",
      "0.8166666666666667\n",
      "0.819672131147541\n",
      "0.8064516129032258\n",
      "0.8095238095238095\n",
      "0.8125\n",
      "0.8\n",
      "0.803030303030303\n",
      "0.8059701492537313\n",
      "0.8088235294117647\n",
      "0.8115942028985508\n",
      "0.8142857142857143\n",
      "0.8169014084507042\n",
      "0.8194444444444444\n",
      "0.821917808219178\n",
      "0.8243243243243243\n",
      "0.8133333333333334\n",
      "0.8157894736842105\n",
      "0.8181818181818182\n",
      "0.8205128205128205\n",
      "0.8227848101265823\n",
      "0.8125\n",
      "0.8148148148148148\n",
      "0.8170731707317073\n",
      "0.8192771084337349\n",
      "0.8214285714285714\n",
      "0.8235294117647058\n",
      "0.813953488372093\n",
      "0.8160919540229885\n",
      "0.8181818181818182\n",
      "0.8202247191011236\n",
      "0.8222222222222222\n",
      "0.8241758241758241\n",
      "0.8260869565217391\n",
      "0.8279569892473119\n",
      "0.8191489361702128\n",
      "0.8105263157894737\n",
      "0.8125\n",
      "0.8144329896907216\n",
      "0.8163265306122449\n",
      "0.8080808080808081\n",
      "0.8\n",
      "0.801980198019802\n",
      "0.803921568627451\n",
      "0.7961165048543689\n",
      "0.7980769230769231\n",
      "0.8\n",
      "0.8018867924528302\n",
      "0.794392523364486\n",
      "0.7962962962962963\n",
      "0.7889908256880734\n",
      "0.7909090909090909\n",
      "0.7927927927927928\n",
      "0.7946428571428571\n",
      "0.7964601769911505\n",
      "0.7894736842105263\n",
      "0.7913043478260869\n",
      "0.7931034482758621\n",
      "0.7863247863247863\n",
      "0.788135593220339\n",
      "0.7899159663865546\n",
      "0.7833333333333333\n",
      "0.7768595041322314\n",
      "0.7786885245901639\n",
      "0.7804878048780488\n",
      "0.782258064516129\n",
      "0.776\n",
      "0.7698412698412699\n",
      "0.7716535433070866\n",
      "0.7734375\n",
      "0.7751937984496124\n",
      "0.7769230769230769\n",
      "0.7786259541984732\n",
      "0.7803030303030303\n",
      "0.7819548872180451\n",
      "0.7835820895522388\n",
      "0.7851851851851852\n",
      "0.7867647058823529\n",
      "0.7883211678832117\n",
      "0.7898550724637681\n",
      "0.7913669064748201\n",
      "0.7928571428571428\n",
      "0.7943262411347518\n",
      "0.795774647887324\n",
      "0.7972027972027972\n",
      "0.7916666666666666\n",
      "0.7931034482758621\n",
      "0.7945205479452054\n",
      "0.7959183673469388\n",
      "0.7972972972972973\n",
      "0.7986577181208053\n",
      "0.8\n",
      "0.8013245033112583\n",
      "0.8026315789473685\n",
      "0.803921568627451\n",
      "0.8051948051948052\n",
      "0.8064516129032258\n",
      "0.8076923076923077\n",
      "0.8089171974522293\n",
      "0.810126582278481\n",
      "0.8113207547169812\n",
      "0.8125\n",
      "0.8136645962732919\n",
      "0.8148148148148148\n",
      "0.8098159509202454\n",
      "0.8109756097560976\n",
      "0.806060606060606\n",
      "0.8072289156626506\n",
      "0.8083832335329342\n",
      "0.8095238095238095\n",
      "0.8106508875739645\n",
      "0.8117647058823529\n",
      "0.8128654970760234\n",
      "0.813953488372093\n",
      "0.815028901734104\n",
      "0.8160919540229885\n",
      "0.8114285714285714\n",
      "0.8125\n",
      "0.8135593220338984\n",
      "0.8146067415730337\n",
      "0.8156424581005587\n",
      "0.8166666666666667\n",
      "0.8176795580110497\n",
      "0.8186813186813187\n",
      "0.8142076502732241\n",
      "0.8152173913043478\n",
      "0.8162162162162162\n",
      "0.8172043010752689\n",
      "0.8128342245989305\n",
      "0.8138297872340425\n",
      "0.8148148148148148\n",
      "0.8157894736842105\n",
      "0.8167539267015707\n",
      "0.8177083333333334\n",
      "0.8134715025906736\n",
      "0.8144329896907216\n",
      "0.8153846153846154\n",
      "0.8163265306122449\n",
      "0.817258883248731\n",
      "0.8131313131313131\n",
      "0.8140703517587939\n",
      "0.815\n",
      "0.8109452736318408\n",
      "0.8118811881188119\n",
      "0.812807881773399\n",
      "0.8137254901960784\n",
      "0.8146341463414634\n",
      "0.8155339805825242\n",
      "0.8164251207729468\n",
      "0.8125\n",
      "0.8133971291866029\n",
      "0.8095238095238095\n",
      "0.8104265402843602\n",
      "0.8113207547169812\n",
      "0.812206572769953\n",
      "0.8130841121495327\n",
      "0.813953488372093\n",
      "0.8148148148148148\n",
      "0.815668202764977\n",
      "0.8165137614678899\n",
      "0.817351598173516\n",
      "0.8181818181818182\n",
      "0.8144796380090498\n",
      "0.8153153153153153\n",
      "0.8161434977578476\n",
      "0.8125\n",
      "0.8088888888888889\n",
      "0.8097345132743363\n",
      "0.8061674008810573\n",
      "0.8070175438596491\n",
      "0.8078602620087336\n",
      "0.808695652173913\n",
      "0.8051948051948052\n",
      "0.8060344827586207\n",
      "0.8068669527896996\n",
      "0.8076923076923077\n",
      "0.8042553191489362\n",
      "0.8050847457627118\n",
      "0.8016877637130801\n",
      "0.8025210084033614\n",
      "0.803347280334728\n",
      "0.8041666666666667\n",
      "0.8049792531120332\n",
      "0.8057851239669421\n",
      "0.8065843621399177\n",
      "0.8073770491803278\n",
      "0.8081632653061225\n",
      "0.8089430894308943\n",
      "0.805668016194332\n",
      "0.8064516129032258\n",
      "0.8032128514056225\n",
      "0.804\n",
      "0.8047808764940239\n",
      "0.8055555555555556\n",
      "0.8063241106719368\n",
      "0.8070866141732284\n",
      "0.807843137254902\n",
      "0.80859375\n",
      "0.8093385214007782\n",
      "0.810077519379845\n",
      "0.8108108108108109\n",
      "0.8115384615384615\n",
      "0.8084291187739464\n",
      "0.8091603053435115\n",
      "0.8098859315589354\n",
      "0.8068181818181818\n",
      "0.8075471698113208\n",
      "0.8045112781954887\n",
      "0.8052434456928839\n",
      "0.8059701492537313\n",
      "0.8066914498141264\n",
      "0.8074074074074075\n",
      "0.8081180811808119\n",
      "0.8088235294117647\n",
      "0.8095238095238095\n",
      "0.8102189781021898\n",
      "0.8072727272727273\n",
      "0.8079710144927537\n",
      "0.8050541516245487\n",
      "0.8057553956834532\n",
      "0.8028673835125448\n",
      "0.8\n",
      "0.800711743772242\n",
      "0.8014184397163121\n",
      "0.8021201413427562\n",
      "0.8028169014084507\n",
      "0.8035087719298246\n",
      "0.8041958041958042\n",
      "0.8048780487804879\n",
      "0.8055555555555556\n",
      "0.8027681660899654\n",
      "0.803448275862069\n",
      "0.8006872852233677\n",
      "0.8013698630136986\n",
      "0.8020477815699659\n",
      "0.8027210884353742\n",
      "0.8033898305084746\n",
      "0.8040540540540541\n",
      "0.8047138047138047\n",
      "0.802013422818792\n",
      "0.802675585284281\n",
      "0.8033333333333333\n",
      "0.8039867109634552\n",
      "0.8013245033112583\n",
      "0.801980198019802\n",
      "0.8026315789473685\n",
      "0.8032786885245902\n",
      "0.803921568627451\n",
      "0.8013029315960912\n",
      "0.7987012987012987\n",
      "0.7993527508090615\n",
      "0.8\n",
      "0.8006430868167203\n",
      "0.7980769230769231\n",
      "0.7987220447284346\n",
      "0.7993630573248408\n",
      "0.8\n",
      "0.8006329113924051\n",
      "0.8012618296529969\n",
      "0.8018867924528302\n",
      "0.799373040752351\n",
      "0.8\n",
      "0.8006230529595015\n",
      "0.8012422360248447\n",
      "0.8018575851393189\n",
      "0.8024691358024691\n",
      "0.803076923076923\n",
      "0.803680981595092\n",
      "0.8042813455657493\n",
      "0.8048780487804879\n",
      "0.8054711246200608\n",
      "0.806060606060606\n",
      "0.8066465256797583\n",
      "0.8072289156626506\n",
      "0.8078078078078078\n",
      "0.8083832335329342\n",
      "0.808955223880597\n",
      "0.8095238095238095\n",
      "0.8100890207715133\n",
      "0.8106508875739645\n",
      "0.8112094395280236\n",
      "0.8117647058823529\n",
      "0.8123167155425219\n",
      "0.8128654970760234\n",
      "0.8134110787172012\n",
      "0.813953488372093\n",
      "0.8144927536231884\n",
      "0.8121387283236994\n",
      "0.8126801152737753\n",
      "0.8132183908045977\n",
      "0.8137535816618912\n",
      "0.8142857142857143\n",
      "0.8148148148148148\n",
      "0.8153409090909091\n",
      "0.8158640226628895\n",
      "0.8135593220338984\n",
      "0.8140845070422535\n",
      "0.8146067415730337\n",
      "0.8151260504201681\n",
      "0.8156424581005587\n",
      "0.8161559888579387\n",
      "0.8166666666666667\n",
      "0.817174515235457\n",
      "0.8176795580110497\n",
      "0.8181818181818182\n",
      "0.8186813186813187\n",
      "0.8191780821917808\n",
      "0.819672131147541\n",
      "0.8201634877384196\n",
      "0.8206521739130435\n",
      "0.8211382113821138\n",
      "0.8189189189189189\n",
      "0.8194070080862533\n",
      "0.8198924731182796\n",
      "0.8203753351206434\n",
      "0.8181818181818182\n",
      "0.816\n",
      "0.8164893617021277\n",
      "0.8143236074270557\n",
      "0.8148148148148148\n",
      "0.8153034300791556\n",
      "0.8157894736842105\n",
      "0.8162729658792651\n",
      "0.8141361256544503\n",
      "0.814621409921671\n",
      "0.8151041666666666\n",
      "0.8155844155844156\n",
      "0.8160621761658031\n",
      "0.813953488372093\n",
      "0.8144329896907216\n",
      "0.8149100257069408\n",
      "0.8153846153846154\n",
      "0.8158567774936062\n",
      "0.8163265306122449\n",
      "0.816793893129771\n",
      "0.817258883248731\n",
      "0.8177215189873418\n",
      "0.8156565656565656\n",
      "0.8161209068010076\n",
      "0.8165829145728644\n",
      "0.8145363408521303\n",
      "0.8125\n",
      "0.8129675810473815\n",
      "0.8109452736318408\n",
      "0.8114143920595533\n",
      "0.8118811881188119\n",
      "0.8123456790123457\n",
      "0.812807881773399\n",
      "0.8132678132678133\n",
      "0.8137254901960784\n",
      "0.8117359413202934\n",
      "0.8121951219512196\n",
      "0.8102189781021898\n",
      "0.8106796116504854\n",
      "0.8111380145278451\n",
      "0.8115942028985508\n",
      "0.8120481927710843\n",
      "0.8100961538461539\n",
      "0.8081534772182254\n",
      "0.8086124401913876\n",
      "0.8090692124105012\n",
      "0.8095238095238095\n",
      "0.8076009501187649\n",
      "0.8080568720379147\n",
      "0.8085106382978723\n",
      "0.8066037735849056\n",
      "0.8047058823529412\n",
      "0.8051643192488263\n",
      "0.8056206088992974\n",
      "0.8060747663551402\n",
      "0.8041958041958042\n",
      "0.8046511627906977\n",
      "0.8051044083526682\n",
      "0.8055555555555556\n",
      "0.8036951501154734\n",
      "0.804147465437788\n",
      "0.8045977011494253\n",
      "0.805045871559633\n",
      "0.8054919908466819\n",
      "0.8059360730593608\n",
      "0.806378132118451\n",
      "0.8068181818181818\n",
      "0.8072562358276644\n",
      "0.8054298642533937\n",
      "0.8058690744920993\n",
      "0.8063063063063063\n",
      "0.8067415730337079\n",
      "0.8071748878923767\n",
      "0.8076062639821029\n",
      "0.8080357142857143\n",
      "0.8084632516703786\n",
      "0.8088888888888889\n",
      "0.8093126385809313\n",
      "0.8075221238938053\n",
      "0.8079470198675497\n",
      "0.8083700440528634\n",
      "0.8065934065934066\n",
      "0.8070175438596491\n",
      "0.8074398249452954\n",
      "0.8078602620087336\n",
      "0.8061002178649237\n",
      "0.8065217391304348\n",
      "0.806941431670282\n",
      "0.8073593073593074\n",
      "0.8077753779697624\n",
      "0.8081896551724138\n",
      "0.8086021505376344\n",
      "0.8090128755364807\n",
      "0.8094218415417559\n",
      "0.8076923076923077\n",
      "0.8081023454157783\n",
      "0.8085106382978723\n",
      "0.8089171974522293\n",
      "0.809322033898305\n",
      "0.8097251585623678\n",
      "0.810126582278481\n",
      "0.8105263157894737\n",
      "0.8109243697478992\n",
      "0.8113207547169812\n",
      "0.8096234309623431\n",
      "0.8079331941544885\n",
      "0.80625\n",
      "0.8066528066528067\n",
      "0.8070539419087137\n",
      "0.8074534161490683\n",
      "0.8078512396694215\n",
      "0.8082474226804124\n",
      "0.8065843621399177\n",
      "0.8069815195071869\n",
      "0.805327868852459\n",
      "0.8057259713701431\n",
      "0.8061224489795918\n",
      "0.8065173116089613\n",
      "0.806910569105691\n",
      "0.8073022312373225\n",
      "0.8076923076923077\n",
      "0.8080808080808081\n",
      "0.8084677419354839\n",
      "0.8088531187122736\n",
      "0.8092369477911646\n",
      "0.8096192384769539\n",
      "0.81\n",
      "0.810379241516966\n",
      "0.8107569721115537\n",
      "0.8111332007952287\n",
      "0.8115079365079365\n",
      "0.8118811881188119\n",
      "0.8122529644268774\n",
      "0.8126232741617357\n",
      "0.812992125984252\n",
      "0.8133595284872298\n",
      "0.8117647058823529\n",
      "0.812133072407045\n",
      "0.810546875\n",
      "0.8109161793372319\n",
      "0.811284046692607\n",
      "0.8097087378640777\n",
      "0.8081395348837209\n",
      "0.8085106382978723\n",
      "0.8088803088803089\n",
      "0.8073217726396917\n",
      "0.8076923076923077\n",
      "0.8080614203454894\n",
      "0.8084291187739464\n",
      "0.8087954110898662\n",
      "0.8072519083969466\n",
      "0.8057142857142857\n",
      "0.8060836501901141\n",
      "0.8064516129032258\n",
      "0.8068181818181818\n",
      "0.8052930056710775\n",
      "0.8056603773584906\n",
      "0.8060263653483992\n",
      "0.806390977443609\n",
      "0.8067542213883677\n",
      "0.8071161048689138\n",
      "0.8074766355140187\n",
      "0.8078358208955224\n",
      "0.8081936685288641\n",
      "0.8085501858736059\n",
      "0.8089053803339518\n",
      "0.8092592592592592\n",
      "0.8096118299445472\n",
      "0.8099630996309963\n",
      "0.8103130755064457\n",
      "0.8106617647058824\n",
      "0.8110091743119267\n",
      "0.8113553113553114\n",
      "0.8117001828153565\n",
      "0.8102189781021898\n",
      "0.8105646630236795\n",
      "0.8109090909090909\n",
      "0.8112522686025408\n",
      "0.8115942028985508\n",
      "0.810126582278481\n",
      "0.8086642599277978\n",
      "0.8072072072072072\n",
      "0.8075539568345323\n",
      "0.8078994614003591\n",
      "0.8082437275985663\n",
      "0.8067978533094812\n",
      "0.8071428571428572\n",
      "0.8057040998217468\n",
      "0.806049822064057\n",
      "0.8063943161634103\n",
      "0.8067375886524822\n",
      "0.8070796460176991\n",
      "0.8074204946996466\n",
      "0.8077601410934744\n",
      "0.8080985915492958\n",
      "0.8084358523725835\n",
      "0.8087719298245614\n",
      "0.809106830122592\n",
      "0.8094405594405595\n",
      "0.8097731239092496\n",
      "0.8083623693379791\n",
      "0.808695652173913\n",
      "0.8090277777777778\n",
      "0.8093587521663779\n",
      "0.8096885813148789\n",
      "0.8100172711571675\n",
      "0.8086206896551724\n",
      "0.8089500860585198\n",
      "0.8092783505154639\n",
      "0.8078902229845626\n",
      "0.8082191780821918\n",
      "0.8068376068376069\n",
      "0.8071672354948806\n",
      "0.807495741056218\n",
      "0.8078231292517006\n",
      "0.8081494057724957\n",
      "0.8084745762711865\n",
      "0.8087986463620981\n",
      "0.8091216216216216\n",
      "0.8077571669477235\n",
      "0.8080808080808081\n",
      "0.8084033613445378\n",
      "0.8087248322147651\n",
      "0.8090452261306532\n",
      "0.8093645484949833\n",
      "0.8096828046744574\n",
      "0.81\n",
      "0.8086522462562395\n",
      "0.8089700996677741\n",
      "0.8092868988391376\n",
      "0.8096026490066225\n",
      "0.8082644628099174\n",
      "0.8085808580858086\n",
      "0.8072487644151565\n",
      "0.8075657894736842\n",
      "0.8078817733990148\n",
      "0.8081967213114755\n",
      "0.8085106382978723\n",
      "0.8088235294117647\n",
      "0.8075040783034257\n",
      "0.8061889250814332\n",
      "0.8065040650406504\n",
      "0.8068181818181818\n",
      "0.807131280388979\n",
      "0.8074433656957929\n",
      "0.8077544426494345\n",
      "0.8080645161290323\n",
      "0.8083735909822867\n",
      "0.8086816720257235\n",
      "0.8073836276083467\n",
      "0.8060897435897436\n",
      "0.8048\n",
      "0.805111821086262\n",
      "0.8038277511961722\n",
      "0.804140127388535\n",
      "0.8028616852146264\n",
      "0.8031746031746032\n",
      "0.803486529318542\n",
      "0.8037974683544303\n",
      "0.8041074249605056\n",
      "0.804416403785489\n",
      "0.8047244094488188\n",
      "0.8050314465408805\n",
      "0.8053375196232339\n",
      "0.8056426332288401\n",
      "0.8059467918622848\n",
      "0.8046875\n",
      "0.8049921996879875\n",
      "0.8037383177570093\n",
      "0.8040435458786936\n",
      "0.8043478260869565\n",
      "0.8046511627906977\n",
      "0.804953560371517\n",
      "0.80370942812983\n",
      "0.8040123456790124\n",
      "0.8043143297380585\n",
      "0.8046153846153846\n",
      "0.804915514592934\n",
      "0.8052147239263804\n",
      "0.8055130168453293\n",
      "0.8058103975535168\n",
      "0.8061068702290076\n",
      "0.8064024390243902\n",
      "0.806697108066971\n",
      "0.8054711246200608\n",
      "0.8057663125948407\n",
      "0.8045454545454546\n",
      "0.8048411497730711\n",
      "0.8051359516616314\n",
      "0.8054298642533937\n",
      "0.8057228915662651\n",
      "0.8045112781954887\n",
      "0.8048048048048048\n",
      "0.8050974512743628\n",
      "0.8053892215568862\n",
      "0.8056801195814649\n",
      "0.8044776119402985\n",
      "0.8047690014903129\n",
      "0.8050595238095238\n",
      "0.8053491827637445\n",
      "0.8041543026706232\n",
      "0.8044444444444444\n",
      "0.8032544378698225\n",
      "0.8035450516986706\n",
      "0.8023598820058997\n",
      "0.8026509572901326\n",
      "0.8029411764705883\n",
      "0.801762114537445\n",
      "0.8020527859237536\n",
      "0.8023426061493412\n",
      "0.8026315789473685\n",
      "0.8029197080291971\n",
      "0.8017492711370262\n",
      "0.8020378457059679\n",
      "0.8008720930232558\n",
      "0.8011611030478955\n",
      "0.8014492753623188\n",
      "0.8017366136034733\n",
      "0.8020231213872833\n",
      "0.8023088023088023\n",
      "0.8025936599423631\n",
      "0.8028776978417266\n",
      "0.8031609195402298\n",
      "0.8034433285509326\n",
      "0.8037249283667621\n",
      "0.8025751072961373\n",
      "0.8028571428571428\n",
      "0.8031383737517832\n",
      "0.8034188034188035\n",
      "0.802275960170697\n",
      "0.8011363636363636\n",
      "0.8014184397163121\n",
      "0.8016997167138811\n",
      "0.8005657708628006\n",
      "0.8008474576271186\n",
      "0.8011283497884344\n",
      "0.8014084507042254\n",
      "0.8016877637130801\n",
      "0.8019662921348315\n",
      "0.8022440392706872\n",
      "0.8025210084033614\n",
      "0.8027972027972028\n",
      "0.803072625698324\n",
      "0.803347280334728\n",
      "0.8036211699164345\n",
      "0.803894297635605\n",
      "0.8041666666666667\n",
      "0.8044382801664355\n",
      "0.8047091412742382\n",
      "0.8049792531120332\n",
      "0.8052486187845304\n",
      "0.8055172413793104\n",
      "0.8057851239669421\n",
      "0.8060522696011004\n",
      "0.8063186813186813\n",
      "0.8065843621399177\n",
      "0.8068493150684931\n",
      "0.8071135430916553\n",
      "0.8073770491803278\n",
      "0.8076398362892224\n",
      "0.8079019073569482\n",
      "0.8081632653061225\n",
      "0.8084239130434783\n",
      "0.8086838534599728\n",
      "0.8089430894308943\n",
      "0.8092016238159675\n",
      "0.8094594594594594\n",
      "0.8097165991902834\n",
      "0.8099730458221024\n",
      "0.8102288021534321\n",
      "0.8104838709677419\n",
      "0.810738255033557\n",
      "0.8109919571045576\n",
      "0.8099062918340026\n",
      "0.8101604278074866\n",
      "0.8104138851802403\n",
      "0.8093333333333333\n",
      "0.8095872170439414\n",
      "0.8098404255319149\n",
      "0.8100929614873837\n",
      "0.8103448275862069\n",
      "0.8105960264900662\n",
      "0.8108465608465608\n",
      "0.8110964332892999\n",
      "0.8113456464379947\n",
      "0.8115942028985508\n",
      "0.8118421052631579\n",
      "0.8107752956636005\n",
      "0.8110236220472441\n",
      "0.8112712975098296\n",
      "0.8115183246073299\n",
      "0.8104575163398693\n",
      "0.8107049608355091\n",
      "0.8109517601043025\n",
      "0.8098958333333334\n",
      "0.8101430429128739\n",
      "0.8103896103896104\n",
      "0.8093385214007782\n",
      "0.8095854922279793\n",
      "0.8098318240620958\n",
      "0.810077519379845\n",
      "0.8103225806451613\n",
      "0.8092783505154639\n",
      "0.8095238095238095\n",
      "0.8097686375321337\n",
      "0.8087291399229781\n",
      "0.8076923076923077\n",
      "0.8079385403329066\n",
      "0.8081841432225064\n",
      "0.8084291187739464\n",
      "0.8086734693877551\n",
      "0.8089171974522293\n",
      "0.8091603053435115\n",
      "0.8081321473951716\n",
      "0.8083756345177665\n",
      "0.8086185044359949\n",
      "0.8088607594936709\n",
      "0.809102402022756\n",
      "0.8080808080808081\n",
      "0.8083228247162674\n",
      "0.8085642317380353\n",
      "0.8088050314465409\n",
      "0.8077889447236181\n",
      "0.8080301129234629\n",
      "0.8070175438596491\n",
      "0.8072590738423029\n",
      "0.8075\n",
      "0.8077403245942572\n",
      "0.8079800498753117\n",
      "0.8082191780821918\n",
      "0.8072139303482587\n",
      "0.8074534161490683\n",
      "0.8064516129032258\n",
      "0.8066914498141264\n",
      "0.806930693069307\n",
      "0.8071693448702101\n",
      "0.8074074074074075\n",
      "0.8076448828606658\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, tokenizer, dataset)\u001b[39m\n\u001b[32m     10\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m     11\u001b[39m inp = tokenizer(sample[\u001b[33m\"\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m\"\u001b[39m], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m prediction = tokenizer.decode(output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction == sample[\u001b[33m\"\u001b[39m\u001b[33mtarget_text\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1764\u001b[39m, in \u001b[36mT5ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1761\u001b[39m         decoder_attention_mask = decoder_attention_mask.to(\u001b[38;5;28mself\u001b[39m.decoder.first_device)\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m sequence_output = decoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1100\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m   1098\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1118\u001b[39m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:711\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    709\u001b[39m do_cross_attention = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     cross_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m     hidden_states = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:640\u001b[39m, in \u001b[36mT5LayerCrossAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, query_length, output_attentions, cache_position)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    627\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    637\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    638\u001b[39m ):\n\u001b[32m    639\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m     layer_output = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n\u001b[32m    653\u001b[39m     outputs = (layer_output,) + attention_output[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:514\u001b[39m, in \u001b[36mT5Attention.forward\u001b[39m\u001b[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m     key_states = \u001b[38;5;28mself\u001b[39m.k(current_states)\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     value_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     key_states = key_states.view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.key_value_proj_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    516\u001b[39m     value_states = value_states.view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.key_value_proj_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(model, tokenizer, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model-small\", safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-ecommerce-py3.11 (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
