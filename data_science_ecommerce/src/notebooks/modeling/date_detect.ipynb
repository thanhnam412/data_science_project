{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d995ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils.path_converter import path_converter\n",
    "\n",
    "df = pd.read_csv(path_converter(\"/data/raw/dates_advanced_25k.csv\"))\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.to_csv(path_converter(\"/data/raw/train_dates.csv\"), index=False)\n",
    "val_df.to_csv(path_converter(\"/data/raw/val_dates.csv\"), index=False)\n",
    "test_df.to_csv(path_converter(\"/data/raw/test_dates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e54987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cps/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Generating train split: 20250 examples [00:00, 533187.63 examples/s]\n",
      "Generating val split: 2250 examples [00:00, 646736.84 examples/s]\n",
      "Generating test split: 2500 examples [00:00, 783806.25 examples/s]\n",
      "Map: 100%|██████████| 20250/20250 [00:02<00:00, 7306.36 examples/s]\n",
      "Map: 100%|██████████| 2250/2250 [00:00<00:00, 7507.13 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 7042.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], max_length=64, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"target_text\"], max_length=16, padding=\"max_length\", truncation=True\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": path_converter(\"/data/raw/train_dates.csv\"),\n",
    "        \"val\": path_converter(\"/data/raw/val_dates.csv\"),\n",
    "        \"test\": path_converter(\"/data/raw/test_dates.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223822aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cps/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2532' max='2532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2532/2532 1:56:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.503100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2532, training_loss=0.13033619198188962, metrics={'train_runtime': 7010.56, 'train_samples_per_second': 11.554, 'train_steps_per_second': 0.361, 'total_flos': 1370335739904000.0, 'train_loss': 0.13033619198188962, 'epoch': 4.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"date-normalizer\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=200,\n",
    "    use_cpu=True,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321ccdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-12-09\n"
     ]
    }
   ],
   "source": [
    "def normalize_date(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=20, num_beams=5)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(normalize_date(\"2011-12-09 12:50:00\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57fff804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2011-12-09\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\"2011-12-09 12:50:00\"])\n",
    "df.columns = ['date']\n",
    "\n",
    "df = pd.to_datetime(df['date'])\n",
    "print(df.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8f0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    i = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        i += 1\n",
    "        inp = tokenizer(sample[\"input_text\"], return_tensors=\"pt\")\n",
    "        output = model.generate(**inp, max_length=20)\n",
    "        prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        if prediction == sample[\"target_text\"]:\n",
    "            correct += 1\n",
    "        print(correct / i)\n",
    "\n",
    "    print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd58c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.3333333333333333\n",
      "0.25\n",
      "0.4\n",
      "0.3333333333333333\n",
      "0.42857142857142855\n",
      "0.5\n",
      "0.5555555555555556\n",
      "0.5\n",
      "0.45454545454545453\n",
      "0.5\n",
      "0.5384615384615384\n",
      "0.5714285714285714\n",
      "0.6\n",
      "0.625\n",
      "0.6470588235294118\n",
      "0.6666666666666666\n",
      "0.6842105263157895\n",
      "0.7\n",
      "0.7142857142857143\n",
      "0.7272727272727273\n",
      "0.7391304347826086\n",
      "0.7083333333333334\n",
      "0.72\n",
      "0.6923076923076923\n",
      "0.7037037037037037\n",
      "0.7142857142857143\n",
      "0.7241379310344828\n",
      "0.7333333333333333\n",
      "0.7419354838709677\n",
      "0.75\n",
      "0.7575757575757576\n",
      "0.7647058823529411\n",
      "0.7714285714285715\n",
      "0.75\n",
      "0.7297297297297297\n",
      "0.7105263157894737\n",
      "0.717948717948718\n",
      "0.725\n",
      "0.7317073170731707\n",
      "0.7380952380952381\n",
      "0.7441860465116279\n",
      "0.75\n",
      "0.7555555555555555\n",
      "0.7391304347826086\n",
      "0.723404255319149\n",
      "0.7291666666666666\n",
      "0.7346938775510204\n",
      "0.74\n",
      "0.7450980392156863\n",
      "0.75\n",
      "0.7547169811320755\n",
      "0.7592592592592593\n",
      "0.7636363636363637\n",
      "0.7678571428571429\n",
      "0.7543859649122807\n",
      "0.7586206896551724\n",
      "0.7627118644067796\n",
      "0.7666666666666667\n",
      "0.7704918032786885\n",
      "0.7741935483870968\n",
      "0.7777777777777778\n",
      "0.78125\n",
      "0.7692307692307693\n",
      "0.7727272727272727\n",
      "0.7761194029850746\n",
      "0.7794117647058824\n",
      "0.782608695652174\n",
      "0.7857142857142857\n",
      "0.7887323943661971\n",
      "0.7916666666666666\n",
      "0.7808219178082192\n",
      "0.7702702702702703\n",
      "0.7733333333333333\n",
      "0.7763157894736842\n",
      "0.7792207792207793\n",
      "0.782051282051282\n",
      "0.7848101265822784\n",
      "0.7875\n",
      "0.7901234567901234\n",
      "0.7804878048780488\n",
      "0.7831325301204819\n",
      "0.7738095238095238\n",
      "0.7764705882352941\n",
      "0.7674418604651163\n",
      "0.7701149425287356\n",
      "0.7727272727272727\n",
      "0.7752808988764045\n",
      "0.7666666666666667\n",
      "0.7692307692307693\n",
      "0.7608695652173914\n",
      "0.7526881720430108\n",
      "0.7553191489361702\n",
      "0.7578947368421053\n",
      "0.7604166666666666\n",
      "0.7628865979381443\n",
      "0.7653061224489796\n",
      "0.7676767676767676\n",
      "0.77\n",
      "0.7623762376237624\n",
      "0.7549019607843137\n",
      "0.7572815533980582\n",
      "0.7596153846153846\n",
      "0.7523809523809524\n",
      "0.7547169811320755\n",
      "0.7570093457943925\n",
      "0.7592592592592593\n",
      "0.7522935779816514\n",
      "0.7545454545454545\n",
      "0.7567567567567568\n",
      "0.7589285714285714\n",
      "0.7522123893805309\n",
      "0.7543859649122807\n",
      "0.7565217391304347\n",
      "0.75\n",
      "0.7521367521367521\n",
      "0.7542372881355932\n",
      "0.7563025210084033\n",
      "0.7583333333333333\n",
      "0.7520661157024794\n",
      "0.7540983606557377\n",
      "0.7560975609756098\n",
      "0.7580645161290323\n",
      "0.76\n",
      "0.7619047619047619\n",
      "0.7637795275590551\n",
      "0.765625\n",
      "0.7674418604651163\n",
      "0.7692307692307693\n",
      "0.7709923664122137\n",
      "0.7727272727272727\n",
      "0.7669172932330827\n",
      "0.7686567164179104\n",
      "0.7703703703703704\n",
      "0.7647058823529411\n",
      "0.7664233576642335\n",
      "0.7681159420289855\n",
      "0.7697841726618705\n",
      "0.7714285714285715\n",
      "0.7730496453900709\n",
      "0.7746478873239436\n",
      "0.7762237762237763\n",
      "0.7777777777777778\n",
      "0.7793103448275862\n",
      "0.7808219178082192\n",
      "0.7755102040816326\n",
      "0.777027027027027\n",
      "0.7785234899328859\n",
      "0.78\n",
      "0.7814569536423841\n",
      "0.7763157894736842\n",
      "0.7777777777777778\n",
      "0.7727272727272727\n",
      "0.7677419354838709\n",
      "0.7628205128205128\n",
      "0.7643312101910829\n",
      "0.7658227848101266\n",
      "0.7672955974842768\n",
      "0.7625\n",
      "0.7639751552795031\n",
      "0.7654320987654321\n",
      "0.7668711656441718\n",
      "0.7682926829268293\n",
      "0.7696969696969697\n",
      "0.7650602409638554\n",
      "0.7664670658682635\n",
      "0.7678571428571429\n",
      "0.7692307692307693\n",
      "0.7705882352941177\n",
      "0.7719298245614035\n",
      "0.7674418604651163\n",
      "0.7687861271676301\n",
      "0.764367816091954\n",
      "0.7657142857142857\n",
      "0.7670454545454546\n",
      "0.768361581920904\n",
      "0.7696629213483146\n",
      "0.770949720670391\n",
      "0.7722222222222223\n",
      "0.7679558011049724\n",
      "0.7637362637362637\n",
      "0.7650273224043715\n",
      "0.7663043478260869\n",
      "0.7675675675675676\n",
      "0.7688172043010753\n",
      "0.7700534759358288\n",
      "0.7659574468085106\n",
      "0.7671957671957672\n",
      "0.7684210526315789\n",
      "0.7696335078534031\n",
      "0.765625\n",
      "0.7668393782383419\n",
      "0.7680412371134021\n",
      "0.7692307692307693\n",
      "0.7704081632653061\n",
      "0.7715736040609137\n",
      "0.7727272727272727\n",
      "0.7738693467336684\n",
      "0.775\n",
      "0.7761194029850746\n",
      "0.7772277227722773\n",
      "0.7783251231527094\n",
      "0.7794117647058824\n",
      "0.775609756097561\n",
      "0.7718446601941747\n",
      "0.7681159420289855\n",
      "0.7692307692307693\n",
      "0.7655502392344498\n",
      "0.7619047619047619\n",
      "0.7630331753554502\n",
      "0.7641509433962265\n",
      "0.7652582159624414\n",
      "0.7616822429906542\n",
      "0.7627906976744186\n",
      "0.7638888888888888\n",
      "0.7603686635944701\n",
      "0.7568807339449541\n",
      "0.7579908675799086\n",
      "0.759090909090909\n",
      "0.7601809954751131\n",
      "0.7612612612612613\n",
      "0.7623318385650224\n",
      "0.7589285714285714\n",
      "0.76\n",
      "0.7610619469026548\n",
      "0.762114537444934\n",
      "0.7631578947368421\n",
      "0.7641921397379913\n",
      "0.7652173913043478\n",
      "0.7662337662337663\n",
      "0.7629310344827587\n",
      "0.7639484978540773\n",
      "0.7649572649572649\n",
      "0.7659574468085106\n",
      "0.7669491525423728\n",
      "0.7679324894514767\n",
      "0.7689075630252101\n",
      "0.7698744769874477\n",
      "0.7708333333333334\n",
      "0.7717842323651453\n",
      "0.7727272727272727\n",
      "0.7736625514403292\n",
      "0.7745901639344263\n",
      "0.7755102040816326\n",
      "0.7764227642276422\n",
      "0.7773279352226721\n",
      "0.7782258064516129\n",
      "0.7791164658634538\n",
      "0.78\n",
      "0.7808764940239044\n",
      "0.7777777777777778\n",
      "0.7786561264822134\n",
      "0.7795275590551181\n",
      "0.7803921568627451\n",
      "0.78125\n",
      "0.7821011673151751\n",
      "0.7829457364341085\n",
      "0.7799227799227799\n",
      "0.7807692307692308\n",
      "0.7816091954022989\n",
      "0.7824427480916031\n",
      "0.7832699619771863\n",
      "0.7840909090909091\n",
      "0.7811320754716982\n",
      "0.7781954887218046\n",
      "0.7790262172284644\n",
      "0.7798507462686567\n",
      "0.7806691449814126\n",
      "0.7777777777777778\n",
      "0.7785977859778598\n",
      "0.7794117647058824\n",
      "0.7802197802197802\n",
      "0.781021897810219\n",
      "0.7818181818181819\n",
      "0.7789855072463768\n",
      "0.776173285198556\n",
      "0.7769784172661871\n",
      "0.7777777777777778\n",
      "0.7785714285714286\n",
      "0.7793594306049823\n",
      "0.7801418439716312\n",
      "0.7809187279151943\n",
      "0.778169014084507\n",
      "0.7789473684210526\n",
      "0.7762237762237763\n",
      "0.7770034843205574\n",
      "0.7777777777777778\n",
      "0.7750865051903114\n",
      "0.7758620689655172\n",
      "0.7766323024054983\n",
      "0.7773972602739726\n",
      "0.7781569965870307\n",
      "0.7789115646258503\n",
      "0.7796610169491526\n",
      "0.7804054054054054\n",
      "0.7811447811447811\n",
      "0.7818791946308725\n",
      "0.782608695652174\n",
      "0.7833333333333333\n",
      "0.7807308970099668\n",
      "0.7814569536423841\n",
      "0.7821782178217822\n",
      "0.7828947368421053\n",
      "0.7836065573770492\n",
      "0.7843137254901961\n",
      "0.7850162866449512\n",
      "0.7857142857142857\n",
      "0.7864077669902912\n",
      "0.7870967741935484\n",
      "0.7877813504823151\n",
      "0.7884615384615384\n",
      "0.7891373801916933\n",
      "0.7898089171974523\n",
      "0.7904761904761904\n",
      "0.7911392405063291\n",
      "0.7917981072555205\n",
      "0.7924528301886793\n",
      "0.7931034482758621\n",
      "0.79375\n",
      "0.794392523364486\n",
      "0.7950310559006211\n",
      "0.7956656346749226\n",
      "0.7962962962962963\n",
      "0.7938461538461539\n",
      "0.7944785276073619\n",
      "0.7920489296636085\n",
      "0.7926829268292683\n",
      "0.7933130699088146\n",
      "0.7909090909090909\n",
      "0.7915407854984894\n",
      "0.7891566265060241\n",
      "0.7897897897897898\n",
      "0.7904191616766467\n",
      "0.7880597014925373\n",
      "0.7886904761904762\n",
      "0.7893175074183977\n",
      "0.7899408284023669\n",
      "0.7905604719764012\n",
      "0.7911764705882353\n",
      "0.7888563049853372\n",
      "0.7865497076023392\n",
      "0.7842565597667639\n",
      "0.7819767441860465\n",
      "0.7797101449275362\n",
      "0.7774566473988439\n",
      "0.7780979827089337\n",
      "0.7787356321839081\n",
      "0.7793696275071633\n",
      "0.78\n",
      "0.7806267806267806\n",
      "0.7784090909090909\n",
      "0.7790368271954674\n",
      "0.7796610169491526\n",
      "0.780281690140845\n",
      "0.7808988764044944\n",
      "0.7815126050420168\n",
      "0.7821229050279329\n",
      "0.7827298050139275\n",
      "0.7833333333333333\n",
      "0.7839335180055401\n",
      "0.7845303867403315\n",
      "0.7851239669421488\n",
      "0.782967032967033\n",
      "0.7835616438356164\n",
      "0.7841530054644809\n",
      "0.782016348773842\n",
      "0.782608695652174\n",
      "0.7831978319783198\n",
      "0.7837837837837838\n",
      "0.784366576819407\n",
      "0.7849462365591398\n",
      "0.7855227882037533\n",
      "0.786096256684492\n",
      "0.7866666666666666\n",
      "0.7872340425531915\n",
      "0.7877984084880637\n",
      "0.7883597883597884\n",
      "0.7889182058047494\n",
      "0.7894736842105263\n",
      "0.7900262467191601\n",
      "0.7905759162303665\n",
      "0.7911227154046997\n",
      "0.7890625\n",
      "0.7896103896103897\n",
      "0.7901554404145078\n",
      "0.7881136950904393\n",
      "0.788659793814433\n",
      "0.7892030848329049\n",
      "0.7871794871794872\n",
      "0.7877237851662404\n",
      "0.7882653061224489\n",
      "0.7888040712468194\n",
      "0.7893401015228426\n",
      "0.7873417721518987\n",
      "0.7878787878787878\n",
      "0.7884130982367759\n",
      "0.7864321608040201\n",
      "0.7869674185463659\n",
      "0.7875\n",
      "0.7880299251870324\n",
      "0.7885572139303483\n",
      "0.7890818858560794\n",
      "0.7896039603960396\n",
      "0.7901234567901234\n",
      "0.7906403940886699\n",
      "0.7911547911547911\n",
      "0.7916666666666666\n",
      "0.7921760391198044\n",
      "0.7926829268292683\n",
      "0.7931873479318735\n",
      "0.7912621359223301\n",
      "0.7917675544794189\n",
      "0.7922705314009661\n",
      "0.7927710843373494\n",
      "0.7932692307692307\n",
      "0.7913669064748201\n",
      "0.7894736842105263\n",
      "0.7899761336515513\n",
      "0.7880952380952381\n",
      "0.7885985748218527\n",
      "0.7890995260663507\n",
      "0.789598108747045\n",
      "0.7877358490566038\n",
      "0.788235294117647\n",
      "0.7887323943661971\n",
      "0.7892271662763466\n",
      "0.7897196261682243\n",
      "0.7878787878787878\n",
      "0.7883720930232558\n",
      "0.7888631090487239\n",
      "0.7893518518518519\n",
      "0.789838337182448\n",
      "0.7880184331797235\n",
      "0.7885057471264367\n",
      "0.7889908256880734\n",
      "0.7894736842105263\n",
      "0.7899543378995434\n",
      "0.7904328018223234\n",
      "0.7909090909090909\n",
      "0.7891156462585034\n",
      "0.7895927601809954\n",
      "0.7878103837471784\n",
      "0.786036036036036\n",
      "0.7842696629213484\n",
      "0.7847533632286996\n",
      "0.785234899328859\n",
      "0.7834821428571429\n",
      "0.7839643652561247\n",
      "0.7844444444444445\n",
      "0.7849223946784922\n",
      "0.7853982300884956\n",
      "0.7858719646799117\n",
      "0.7863436123348018\n",
      "0.7868131868131868\n",
      "0.7850877192982456\n",
      "0.7855579868708972\n",
      "0.7860262008733624\n",
      "0.7864923747276689\n",
      "0.7869565217391304\n",
      "0.7874186550976139\n",
      "0.7878787878787878\n",
      "0.7883369330453563\n",
      "0.7887931034482759\n",
      "0.789247311827957\n",
      "0.7875536480686696\n",
      "0.7880085653104925\n",
      "0.7863247863247863\n",
      "0.7867803837953091\n",
      "0.7872340425531915\n",
      "0.7876857749469215\n",
      "0.7860169491525424\n",
      "0.7843551797040169\n",
      "0.7848101265822784\n",
      "0.7852631578947369\n",
      "0.7857142857142857\n",
      "0.7861635220125787\n",
      "0.7866108786610879\n",
      "0.7870563674321504\n",
      "0.7854166666666667\n",
      "0.7858627858627859\n",
      "0.7863070539419087\n",
      "0.7846790890269151\n",
      "0.7830578512396694\n",
      "0.7835051546391752\n",
      "0.7818930041152263\n",
      "0.7823408624229979\n",
      "0.7827868852459017\n",
      "0.7811860940695297\n",
      "0.7816326530612245\n",
      "0.7820773930753564\n",
      "0.782520325203252\n",
      "0.7809330628803245\n",
      "0.7813765182186235\n",
      "0.7797979797979798\n",
      "0.780241935483871\n",
      "0.778672032193159\n",
      "0.7791164658634538\n",
      "0.779559118236473\n",
      "0.778\n",
      "0.7784431137724551\n",
      "0.7788844621513944\n",
      "0.7793240556660039\n",
      "0.7797619047619048\n",
      "0.7801980198019802\n",
      "0.7806324110671937\n",
      "0.7810650887573964\n",
      "0.781496062992126\n",
      "0.7819253438113949\n",
      "0.7803921568627451\n",
      "0.7808219178082192\n",
      "0.78125\n",
      "0.7816764132553606\n",
      "0.7801556420233463\n",
      "0.7805825242718447\n",
      "0.7810077519379846\n",
      "0.781431334622824\n",
      "0.7818532818532818\n",
      "0.7822736030828517\n",
      "0.7826923076923077\n",
      "0.783109404990403\n",
      "0.7835249042145593\n",
      "0.7839388145315488\n",
      "0.7824427480916031\n",
      "0.7828571428571428\n",
      "0.7832699619771863\n",
      "0.7817836812144212\n",
      "0.7821969696969697\n",
      "0.782608695652174\n",
      "0.7830188679245284\n",
      "0.783427495291902\n",
      "0.7838345864661654\n",
      "0.7842401500938087\n",
      "0.7846441947565543\n",
      "0.7850467289719626\n",
      "0.7854477611940298\n",
      "0.7858472998137802\n",
      "0.7862453531598513\n",
      "0.7866419294990723\n",
      "0.7870370370370371\n",
      "0.7874306839186691\n",
      "0.7878228782287823\n",
      "0.7882136279926335\n",
      "0.7886029411764706\n",
      "0.7889908256880734\n",
      "0.7875457875457875\n",
      "0.7861060329067642\n",
      "0.7864963503649635\n",
      "0.7868852459016393\n",
      "0.7872727272727272\n",
      "0.7876588021778584\n",
      "0.7880434782608695\n",
      "0.7884267631103075\n",
      "0.7888086642599278\n",
      "0.7891891891891892\n",
      "0.7877697841726619\n",
      "0.7881508078994613\n",
      "0.7885304659498208\n",
      "0.7871198568872988\n",
      "0.7857142857142857\n",
      "0.786096256684492\n",
      "0.7864768683274022\n",
      "0.7868561278863233\n",
      "0.7872340425531915\n",
      "0.7876106194690266\n",
      "0.7879858657243817\n",
      "0.7883597883597884\n",
      "0.7887323943661971\n",
      "0.789103690685413\n",
      "0.787719298245614\n",
      "0.7880910683012259\n",
      "0.7884615384615384\n",
      "0.7888307155322862\n",
      "0.789198606271777\n",
      "0.7895652173913044\n",
      "0.7899305555555556\n",
      "0.7902946273830156\n",
      "0.7906574394463668\n",
      "0.7910189982728842\n",
      "0.7896551724137931\n",
      "0.7900172117039587\n",
      "0.788659793814433\n",
      "0.7890222984562607\n",
      "0.7876712328767124\n",
      "0.788034188034188\n",
      "0.78839590443686\n",
      "0.7887563884156729\n",
      "0.7874149659863946\n",
      "0.7860780984719864\n",
      "0.7864406779661017\n",
      "0.7868020304568528\n",
      "0.785472972972973\n",
      "0.7858347386172007\n",
      "0.7861952861952862\n",
      "0.7865546218487395\n",
      "0.7869127516778524\n",
      "0.7855946398659966\n",
      "0.7859531772575251\n",
      "0.7863105175292153\n",
      "0.7866666666666666\n",
      "0.7870216306156406\n",
      "0.7857142857142857\n",
      "0.7844112769485904\n",
      "0.7847682119205298\n",
      "0.7851239669421488\n",
      "0.7838283828382838\n",
      "0.7841845140032949\n",
      "0.7845394736842105\n",
      "0.7848932676518884\n",
      "0.7852459016393443\n",
      "0.7855973813420621\n",
      "0.7859477124183006\n",
      "0.7846655791190864\n",
      "0.7850162866449512\n",
      "0.7853658536585366\n",
      "0.7840909090909091\n",
      "0.7844408427876823\n",
      "0.7847896440129449\n",
      "0.7851373182552503\n",
      "0.7838709677419354\n",
      "0.784219001610306\n",
      "0.7845659163987139\n",
      "0.7849117174959872\n",
      "0.7852564102564102\n",
      "0.7856\n",
      "0.7859424920127795\n",
      "0.784688995215311\n",
      "0.7834394904458599\n",
      "0.7837837837837838\n",
      "0.7841269841269841\n",
      "0.7844690966719493\n",
      "0.7832278481012658\n",
      "0.7835703001579779\n",
      "0.7823343848580442\n",
      "0.7826771653543307\n",
      "0.7814465408805031\n",
      "0.7817896389324961\n",
      "0.7821316614420063\n",
      "0.7824726134585289\n",
      "0.7828125\n",
      "0.7831513260530422\n",
      "0.7819314641744548\n",
      "0.7822706065318819\n",
      "0.781055900621118\n",
      "0.7813953488372093\n",
      "0.781733746130031\n",
      "0.7820710973724884\n",
      "0.7824074074074074\n",
      "0.7812018489984591\n",
      "0.7815384615384615\n",
      "0.7818740399385561\n",
      "0.7822085889570553\n",
      "0.7825421133231241\n",
      "0.7813455657492355\n",
      "0.7816793893129771\n",
      "0.7820121951219512\n",
      "0.7823439878234398\n",
      "0.7811550151975684\n",
      "0.779969650986343\n",
      "0.7787878787878788\n",
      "0.7791225416036308\n",
      "0.7794561933534743\n",
      "0.779788838612368\n",
      "0.7786144578313253\n",
      "0.7789473684210526\n",
      "0.7792792792792793\n",
      "0.7796101949025487\n",
      "0.7784431137724551\n",
      "0.7787742899850523\n",
      "0.7791044776119403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, tokenizer, dataset)\u001b[39m\n\u001b[32m     10\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m     11\u001b[39m inp = tokenizer(sample[\u001b[33m\"\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m\"\u001b[39m], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m prediction = tokenizer.decode(output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction == sample[\u001b[33m\"\u001b[39m\u001b[33mtarget_text\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/data-science-ecommerce-cOQ0u3NZ-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:2800\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2796\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2798\u001b[39m \u001b[38;5;66;03m# Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\u001b[39;00m\n\u001b[32m   2799\u001b[39m \u001b[38;5;66;03m# (the clone itself is always small)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2800\u001b[39m next_token_logits = \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2802\u001b[39m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[32m   2803\u001b[39m next_token_scores = logits_processor(input_ids, next_token_logits)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(model, tokenizer, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b2d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model-small\", safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-ecommerce-py3.11 (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
